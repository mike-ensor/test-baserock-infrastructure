Baserock project public infrastructure
======================================

None of these systems are currently Baserock systems, which should be
considered a bug. The need for project infrastructure outweighs the
benefit of using the infrastructure to drive improvements to Baserock,
at the time of writing.

The infrastructure is set up in a way that parallels the preferred Baserock
approach to deployment. All files necessary for (re)deploying the systems
should be contained in this Git repository, with the exception of certain
private tokens (which should be simple to inject at deploy time). Each
service should be provided by a system which services only one function.

I apologise if it's a bit over-complicated! Part of the goal for this work
has been to learn Ansible and Packer, and see how they can be helpful for
Baserock. Feel free to discuss simplifying things which appear overengineered
or needlessly confusing!

Front-end
---------

All of the Baserock project's infrastructure should be behind a single
IP address, with a reverse proxy that forwards the request to the
appropriate machine based on the URL (primarily the subdomain).

The 'frontend' system takes care of this. If you want to add a new
service to the Baserock Project infrastructure you will need to alter
the haproxy.cfg file in the frontend/ directory. OpenStack doesn't
provide any kind of internal DNS service, so you must put the fixed IP
of each instance.

To deploy this system:

    packer build -only=production frontend/packer_template.json

Full HAProxy 1.5 documentation: <https://cbonte.github.io/haproxy-dconv/configuration-1.5.html>.

When setting up a new instance with the frontend already deployed, do the
following:

- request a subdomain that points at 85.199.252.162
- log in to the frontend-haproxy machine
- edit /etc/haproxy.conf as described below, and make the same changes to the
  copy in this repo
- run: `sudo haproxy -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid -sf
  $(cat /var/run/haproxy.pid)` to reload the configuration without interrupting
  the service (this confuses systemd, but I'm not sure how to avoid that)

If I was adding monkeys.baserock.org, this is what I'd add to the config file:

    # (in the 'frontend' section)
        acl host_monkeys hdr(host) -m beg -i monkeys
        use_backend baserock_monkey_service if host_monkeys

    backend baserock_monkey_service
        server baserock_monkey_service 192.168.x.x

Database
--------

Baserock uses MariaDB (a fork of MySQL). Storyboard has database migration
files which are tied to this database, so we do not have the choice of using
others at this time.

The Packer build only creates an image with MariaDB installed. To deploy, you
will need to set up and attach a volume and then start the 'mariadb' service.
Also, you must create or have access to an Ansible playbook which will set up
the user accounts. For development deployments you can use the 'develop.sh'
script which sets up all the necessary accounts using dummy passwords.

To deploy a development instance:

    packer build -only=development database/packer_template.json
    database/develop.sh

To deploy this system to production:

    packer build -only=production database/packer_template.json
    nova boot \
        --flavor dc1.1x1 --image 'database-mariadb' \
        --key-name=<your-keypair> database-mariadb \
        --nic='net-id=d079fa3e-2558-4bcb-ad5a-279040c202b5,v4-fixed-ip=192.168.222.30'
    nova volume-create \
        --display-name database-volume \
        --display-description 'Database volume' \
        10
    nova volume-attach database-mariadb <volume ID> auto

    nova floating-ip-associate database-mariadb <some floating IP>

    # Set up the volume inside the machine
    echo <IP> > dbhost
    ansible \* -i dbhost --user=fedora --sudo -m shell \
        -a "mkfs.ext4 /dev/vdb -L database-volume"
    ansible \* -i dbhost --user=fedora --sudo -m lineinfile \
        -a "dest=/etc/fstab create=yes line='LABEL=database-volume /var/lib/mysql ext4 defaults 1 2'"
    ansible \* -i dbhost --user=fedora --sudo -m shell \
        -a "mount -a"

    # FIXME: here we start the service before setting the root password!!!!
    ansible \* -i dbhost --user=fedora --sudo -m service \
        -a "name=mariadb enabled=true state=started"

    ansible-playbook -i dbhost --user=fedora database/user_config.yml

    nova floating-ip-disassociate database-mariadb <some floating IP>


OpenID provider
---------------

To deploy a development instance:

    packer build -only=development baserock_openid_provider/packer_template.json
    baserock_openid_provider/develop.sh
    # Now you have a root shell inside your container
    cd /srv/baserock_openid_provider
    python ./manage.py runserver 0.0.0.0:80
    # Now you can browse to http://localhost:80/ and see the server.

To deploy this system to production:

    vim baserock_openid_provider/baserock_openid_provider/settings.py
    # Edit the DATABASES['default']['HOST'] to point to the fixed IP of
    # the 'database' machine.
    packer build -only=production database/packer_template.json
    nova boot openid_provider
        --flavor dc1.1x1 --image 'baserock_openid_provider' \
        --key-name=<your-keypair> openid.baserock.org \
        --nic='net-id=d079fa3e-2558-4bcb-ad5a-279040c202b5'

Now you need to SSH into the system (via the frontend system perhaps) and run
the database migrations, before the app will work:

    python /srv/baserock_openid_provider/manage.py migrate

FIXME: I guess this could be done with cloud-init.


Storyboard
----------

We use a slightly adapted version of
<https://github.com/openstack-infra/puppet-storyboard> to deploy Storyboard.

There's no development deployment for Storyboard at this time: the Puppet
script expects to start services using systemd, and that doesn't work by
default in a Docker container.

To deploy the production version:

    packer build -only=production baserock_storyboard/packer_template.json
    nova boot openid_provider
        --flavor dc1.1x1 --image 'baserock_storyboard' \
        --key-name=<your-keypair> storyboard.baserock.org \
        --nic='net-id=d079fa3e-2558-4bcb-ad5a-279040c202b5'


Firehose
--------

Firehose deployment isn't automated yet. Here are instructions on how to do it
manually:

In a 'build' machine in the cloud:

    morph build systems/build-system-x86_64.morph
    morph deploy baserock-firehose/baserock-firehose.morph

Instatiate this image and SSH into it.

    git clone git://git.baserock.org/baserock/baserock/firehose
    git checkout baserock/perryl/install-firehose
    python ./setup.py install 
    adduser firehose -s /bin/false -D
    sudo -u firehose -- ssh-keygen -C 'firehose@baserock.org' -N ''

Create an account on the Gerrit instance where Firehose will be pushing:

    ssh -p 29418 you@gerrit.baserock.org gerrit \
        create-account --group "'Non-Interactive Users'" \
        --ssh-key xxxx --full-name "'Firehose integration tool'" \
        --email='firehose@firehose.baserock.org' \
        firehose

Test that the Firehose user can connect to Gerrit (this should give you a help
message if successful, or a 'Permission denied' error otherwise):

    sudo -u firehose -- ssh -p 29418 firehose@gerrit.baserock.org gerrit

Install the configuration files and systemd units from the baserock-firehose/
directory (FIXME: they're not there yet)

Run:

    systemctl enable firehose.timer
    systemctl start firehose.timer


Deployment to DataCentred
-------------------------

When instantiating a machine that will be public, remember that all operators
who are responsible for security updates and maintenance must be given access
to the machine. This can be done using a post-creation customisation script
that injecting all of their SSH keys: the Baserock Ops team use the file
`baserock-ops-team.cloud-config` from this repo.

The the Packer tool requires a floating IP to be available at the time a system
is being deployed to OpenStack. Currently 85.199.252.152 should be used for
this.  If you specify a floating IP that is in use by an existing instance, you
will steal it for your own instance and probably break one of our web services.
